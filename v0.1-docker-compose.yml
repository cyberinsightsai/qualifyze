
services:
  duckdb:
    build:
      context: .
      dockerfile: Dockerfile.duckdb
    container_name: duckdb-ui
    # Mount local folder with your CSVs and .duckdb files
    volumes:
      - ./duckdb_data:/data
    # Run DuckDB UI serving a specific DB file (created if not exists)
    command: ["duckdb", "/data/warehouse.duckdb", "-ui"]
    ports:
      - "8080:8080"   # DuckDB UI
    restart: unless-stopped

  superset:
    build:
      context: .
      dockerfile: Dockerfile.superset
    container_name: superset
    depends_on:
      - duckdb
    environment:
      # Superset admin bootstrap
      SUPERSET_SECRET_KEY: "pT0z7o3ZC7yqv9pYF4kQZpQ8bC1l6fV2i3mX0yM5nR8sU2aLwKfVgQ=="

      # Optional tuning
      SUPERSET_GUNICORN_WORKERS: "4"
      SUPERSET_GUNICORN_TIMEOUT: "120"

      # Persisted metadata DB (sqlite in volume). For production, use Postgres.
      SQLALCHEMY_DATABASE_URI: "sqlite:////app/superset_home/superset.db"

    volumes:
      # Persist Superset metadata and configs
      - ./superset_home:/app/superset_home
      # Share DuckDB files so Superset can access local DuckDB databases
      - ./duckdb_data:/app/duckdb
      # Entry script
      - ./docker-entrypoint-init.sh:/docker-entrypoint-init.sh
      # Secret key on python file superset_config.py
      - ./superset_config.py:/app/pythonpath/superset_config.py
    ports:
      - "8088:8088"   # Superset UI
    entrypoint: ["/bin/bash", "/docker-entrypoint-init.sh"]
    restart: unless-stopped

